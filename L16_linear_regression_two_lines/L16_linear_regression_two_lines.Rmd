---
title: "Lecture 16:ANCOVA"
date: "`r format(Sys.time(), '%d %B %Y')`"

output:
   html_document:
     theme: flatly
     toc: true
     toc_depth: 4
     number_sections: yes
---    

In this example we fit a linear model with normal errors to some data comprising a linear covariate and a categorical explanatory variable with two levels. This is commonly referred to as ANCOVA (ANalysis of COVAriance) but we prefer to simply class it as one of the foundation models in the class of Generalised Linear Models. We will fit these models using the function `glm()` but you could fit it with `lm()` or `aov()` intstead, in which case you will be given slightly different output. However, one important point to note is that if you use `aov()`, then in this example, the order in which you specify the explanatory variables will produce very different estimates of significance arising from the fact that the data are not balanced and orthogonal, and that `aov()` fits using sequential sums of squared rather than marginal sums of squared.

#ANCOVA - Analysis of covariance has one continous response variable and at least one continous and categorical explanatory vairable. 

EXAMPLE: 
Data file - Grazing.csv
Variables: Root-continous, Seed-continous and Grazing-categorical
Question: Which one of the continuos variables is explanatory and which one is response?
Research question: Does diameter of the top of its rootstock influences weight of the seed produced in grazed or ungrazed plants?
Solution: 
Diameter of the top of rootstock (Root) - explanatory variable (x-axis)
Weight of the seed produced - response variable (y-axis)

There are in theory 6 posible models that might explain these data:
1. Both slopes and intercept are different (two distinct non-parallel lines)
2. Different intercepts and same slope (two parallel)
3. Same intercept different slopes (this would be an unusual model to fit, unless we were very confident that the intercepts were the same. By way of example, this can arise if you are sure that the data and the underlying lines describing them are constrained to converge at the origin $(x=0, y=0)$).
4. No slope and different intercepts (this is essentially a t-test comparing the means between the two groups)
5. One slope and one intercept (a simple linear regression)
6. No slope only one intercept (this is basically the null model, and comprises a single mean and error term to describe the data).

# Housekeeping

Remember that `rm(list=ls())` is not sufficient for a full clean set up of R, and you should use `Ctrl/Cmd-SHIFT-F10` in Rstudio to Restart R cleanly and check that this works at least at the end of your analysis, or periodically during development.

```{r}
# remove (almost) all objects currently held in the R environment
rm(list=ls()) 
```


#Read in your data

Enter or read in your data from a file as a CSV (comma separated) file from Excel

REMEMBER: Choose your working directory and put file inside that directory!

##CODE EXPLANATION:
Data file - grazing.csv
`header=TRUE` - in first row are the variable names (Root, Seed and Grazing). The `stringsAsFactors=FALSE` option prevents automatic conversion from character to factor representation of the categorical variable `Grazing` within the object. Using `read.csv()` rather than `read.table()` automatically specifies the delimiter with 
`sep=","` - means that values in grazing.csv file are separated with comma (look inside .csv data file in Excel)

```{r setup, include=FALSE}
mydata <- read.csv("grazing.csv", header = TRUE, stringsAsFactors = FALSE)
```

#EXPLORE DATA

```{r explore-data}
head(mydata) # Return the First Part of an Object -usually first six observations
head(mydata, n = 12L) # option - name of the Object, n= number of lines you want to see (12) from the start of the Object

# check the structure of the data
str(mydata) # Return complete Object
```

We expect the response variable Seed and the linear covariate Root to both be numerical, but Grazing represents two categories in this experiment and we should convert these to a factor: the factor format is interpreted by various functions including plotting functions and makes life easier (although many functions will internally coerce a character to factor, it is better to do this ourselves). Since the experimental treatment is designed to look at the effect of Grazing over no grazing, I am going to force the baseline level in this factor to be the "Ungrazed" category. This means that our fitted model later on will expclitly tell us what the effect of grazing is, rather than what the effect of "ungrazing" is which is a bit counter-intuitive. **By default, R will specify the order of the levels alphabetically.**

```{r factor-grazing}

# coerce to factor and specify the order of the levels
mydata$Grazing <- factor(mydata$Grazing, levels = c("Ungrazed", "Grazed"))

# check that this came out as expected.
str(mydata) 

# if you want you can just pull out the levels only
#levels(mydata)
```


#Visualise our data with a scatterplot
```{r}

plot(Seed ~ Root, data = mydata,
     col = Grazing, pch = as.numeric(Grazing),
     xlab = "Root", ylab = "Seed", bty = "L", las = 1,
     cex.axis = 1.2, cex.lab = 1.5)

legend("topleft", levels(mydata$Grazing), pch = 1:2, col = 1:2,
       bty = "n", cex = 1.5)
```



#Fit the most complicated model (non-parallel lines)

Our *a priori* assumption here is that both slopes and intercept in Grazed and Ungrazed group differ! This means that we need to esimate a total of 4 coefficients. Specifically, we fit a model of the form:

$$\text{Seed} = (\beta_0 + \beta_{Grazed}) + (\beta_1 + \beta_{2:Grazed})\text{Root}$$

where $\beta_{Grazed} and \beta_{2:Grazed}$ are only included for data for which `Grazing == Grazed` and are otherwise $0$, in which case the equation simplifies to 

$$\text{Seed} = \beta_0 + \beta_1\text{Root}$$
and represents the line for the Ungrazed plants.

The point of the full equation now is to assess the significance of the term \beta_{2:Grazed} which if different from zero, indicates that the two slopes are different.

**Important:** when fitting models such as this where the slopes differ between two or more categories, it is generally the case that we are left unable to make any meaningful statement about the significance and magnitude of the different between the two intercepts. This occurs because there is no longer a fixed effect of Grazing in this example along the gradient of Root diameter, since non-parallel lines cross at some point, and are infinitely apart (both positively and negatively) at positive and negative infinity. One thing we can do when fitting linear models such as this is to recentre the data by adding or substracting an amount to the explanatory variables. This has the effect of changing the location of the intercepts, but does not affect the estimates of the slopes: meaning that we could end up with differences between the intercepts of the two lines that are either exactly zero, or massively positive or negative depending on our choice. As ever, there are exceptions to this rule, and maybe we want to know if there was a difference between the intercepts at the start of a time-series experiment between two groups, and in that case being able to test the differences betweent he intercepts at a particular value of the explanatory variable might be informative. In the case of this Seed production example though, there is no such obvious insight to be gained.

Specify the interaction in the GLM model

```{r}
model1 <- glm( Seed ~ Root + Grazing + Root:Grazing, data = mydata )

summary(model1)

```

Note that here we have specified the main effects of both `Root` and `Grazing` and manually specified the interaction between the two using the ":"" notation `Root:Grazing`. The short-hand version `glm(See ~ Root * Grazing)` with the "*" notation automatically fits all main and interection effects. This short-hand is fine when we are doing a relatively straight forward model with a few explanatory variables and only simple interactions between pairs of variables, but this can quickly get out of hand with more variables and 3, 4 or even more way interaction terms. Generally it is better, or at least clearer, to specify exactly what you want manually so there are no surprises.

When we analyse coefficients we see that there is no evidence for a significant interaction between Root and Grazing which is the coefficient that determines whether there is a difference between the two slopes. The next step might be to speficy a more simple model with parallel lines, that differ not in their slopes, but only in their intercepts.


#Fit two parallel lines

Here we just test if there are differences in intercept between these two groups. The equation of this model is a more simplified version of the more complicated different slopes above.



```{r}
model2 <- glm( mydata$Seed ~ mydata$Root + mydata$Grazing )

summary(model2)
coefmod2 <- coef(model2)
```

When we analyse coefficients we see that there is difference between Grazed and Ungrazed group while P-value for interaction is lower than 0.05!


#FIT LINES FOR BOTH MODELS

IMPORTANT: ORDERS MATTERS - Grazing variable has two levels Grazed and Ungrazed which are in alphabetical order!

```{r}
plot(mydata$Root, mydata$Seed, xlab="Root",ylab="Seed",col=mydata$Grazing, pch=as.numeric(mydata$Grazing))

legend("topleft", pch=c(2,1), col=c("red", "black"), c("Ungrazed", "Grazed"))
abline (coefmod2[1],coefmod2[2], col="black", lty=1) #First add the line for the Grazed group
abline (coefmod2[1] + coefmod2[3],coefmod2[2], col="red", lty=2 ) #Then add the line for the Ungrazed group
```


#CHECK RESIDUALS OF ANALYSIS

##Extract the residuals for easy recall in model 2 without interaction


```{r}
rsd <- residuals(model2)
```

# Histogram of the residuals

```{r}
hist(rsd,6)
```


## QQ plot of the residuals to assess how well the residuals compare with an ideal normal distribution

```{r}
qqnorm(rsd)
qqline(rsd,col="red")
```



# Check for a trend in the residuals with the X axis variable
```{r}
plot(mydata$Root,rsd)
abline(0,0,col="blue")
```
Any pattern among the differences between model and observed value indicates a failure of the model to adequately represent some aspect of the relationship within the sampled data.

# Save your data 

(only if you want, and this chunk is not evaluated in this markdown)

The "list=" command tells us which variables we want to save
The "file=" option tells us what file to save the data to

```{r, eval = FALSE}
save( list=ls(), file="grazing_data.rdata" )
```


