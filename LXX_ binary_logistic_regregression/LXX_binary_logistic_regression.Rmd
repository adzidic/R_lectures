---
title: "LXX: Binary logistic regression"
date: "`r format(Sys.time(), '%d %B %Y')`"

output:
   html_notebook:
     theme: flatly
     toc: true
     toc_depth: 4
     number_sections: yes
     
---

**_NB THIS FILE NEEDS COMPLETE UPDATING. IT USES THE LINEAR MODELLING FILE BUT RUNS THE BINARY LOGISTIC REGRESSION IN ALMOST A SINGLE CHUNK_**


# Linear regression in R

Fitting lines to data is the cornerstone of statistical modelling. Wherever possible (and there are many tricks that can be employed to help turn curves into straight lines), we try to fit straight lines to data as it is a relatively easy pattern to interpret. Recall the equation of the line is $Y = m X + c$, where $m$ is the slope and is the rate of change of $Y$ with a one unit increase in $X$, and $c$ is the y-axial intercept, or the value of $Y$ when $X = 0$.

In a statistical sense, simple linear regression model describes the effect of the explanatory variable (X) on the response variable (Y) and leaves the data hanging off the line as leftover, or residual error. Explanatory variables may be continuous, discrete, or categorical, but in this simple linear regression we have $X$ as a linear covariate.

We start with the overall equation:

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

where $\beta_0$ is the y-axial intercept (i.e. the value of $Y$ when $X = 0$), and $\beta_1$ is the slope of the linear covariate $X$, and $\epsilon$ is the residual error since no statistical model is perfect.

We can then alter this equation to separate the mean and variance as the representaiton of the error term.

First we can define the expected value for each data point (i.e. their mean) as 

$$\mu_i = \beta_0 + \beta_1 X_i$$

and the error term as 
$$\epsilon_i \sim N(0, \sigma^2)$$
to give

$$Y_i = \mu_i + N(0, \sigma^2)$$

This forumulation lets us easily see that the explanatory variables affect the mean (in statistical terms this means they are **fixed effects**) and the errors are normally distributed centred around 0, and the errors all have the same variance (regardless of their size and regardless of the value of covariate, i.e. they are independent of the fixed part).

The above formulations are somthing of an algebraic statement of the model, but another, perhaps more statistical way of thinking about this is that the data simply have a distribution describing them, which in this case is a normal distribution, and each data point has its own expected value depending on the formula used in the model, and they all have same leftover variance that is not attributed to the fixed part, i.e. the mean.

$$Y_i \sim N(\mu_i, \sigma^2)$$

The data give the diameter of plants in mm and the amount of seed each plant produced in g.

We are interested in how seed production is affected by plant size, as indicated by root diameter.



# Housekeeping

Remember that `rm(list=ls())` is not sufficient for a full clean set up of R, and you should use `Ctrl/Cmd-SHIFT-F10` in Rstudio to Restart R cleanly and check that this works at least at the end of your analysis, or periodically during development.

```{r}
# remove (almost) all objects currently held in the R environment
rm(list=ls()) 
```

# Enter or read in your data from a file

Read in data from our CSV (comma separated value) file

If you need any help for import of the data, take a look at Lesson 3 course material.

Remember that the data file should be in your working directory before you import the file to R.

These data are a slightly modified version taken from data used in Chapter 10 in Crawley 2008, Statistics: An introduction using R. Wiley, ISBN 0-470-02298-1.


```{r}
# load the example data
mydata <- read.csv("binary_example_data.csv", header=TRUE, sep=",")
```


## Whole chunk

```{r}
# --------------------------------------------------------------------
# Plot and explore your data

head(mydata)


# open up a new figure for plotting the raw data
# dev.new()
par(mfrow=c(1,2)) # a 1x2 panel plot

plot(Y ~ X1, type="p",bty="L", yaxp=c(0,1,1),las=1, 
     data = mydata) # a scatter plot

boxplot(X1~Y,horizontal=T,ylab="Y",xlab="X1",
        main="Take care with axes",bty="L",las=1, 
        data = mydata) # a boxplot


# --------------------------------------------------------------------
# Analyse your data
# e.g. a t-test, or linear regression, or ANOVA, or whatever
# 
# NB in this case we specify the family of the GLM to have
# binomial errors (i.e. not the default normal or gaussian errors) for the 
# residuals of the data to the predicted model. Specifically in this case, 
# we are modelling a binomial distribution
# (http://en.wikipedia.org/wiki/Binomial_distribution)
# with one single observation per
# data point (i.e. one coin toss for which record either heads/tails,
# presence/absence, infected/uninfected etc...). This situation is actually a
# special case of the binomial distribution called the bernouli distribution
# http://en.wikipedia.org/wiki/Bernoulli_distribution
model1 <- glm(Y~X1, family=binomial, data = mydata)

summary(model1)


# --------------------------------------------------------------------
# Plot the results of your analysis

# Remember that we have fitted a linear model, and as we have only first-order
# polynomials in X fitted, can expect a straight line on the graph. However,
# you have to remember that this is linear on the log(odds) scale.

# ------------------------------------------------------------------------------
# ** START OF THE 2X2 PANEL PLOTTING **
# ------------------------------------------------------------------------------

# dev.new()
par(mfrow=c(2,2))  # a 2x2 panel plot

# first plot the fitted model on the log(odds) scale on which it operated,
# and on which scale it returns its estimates

plot(0,0,type="n",xlim=c(min(mydata$X1),max(mydata$X1)),ylim=c(-4,4),
      ylab="log(odds)",xlab="X1",main="log(odds) predictions",bty="L")
abline(model1,col="red",lwd=2)

# this is not hugely informative for our data though as we cant add
# the raw data to this plot, since we can calculate log(odds) for stricly 
# binary data as we get 1/(1-1) = 1/0 which is Infinity, and we cant take 
# log(0) which is -Infinity. So, instead we need to plot our data on their
# 0/1 scale and add the model as a probability of Y taking 0 or 1. In order to 
# do this, we need to predict our model for a range of X values. There are two
# ways to acheive this.

# *****************************************************
# PREDICTION METHOD 1 - evaluate the function
# create a new  vector of X values at small increments
X.predict <- seq(0,100,0.01)

b0 <- coef(model1)[1]  # extract the intercept of the model
b1 <- coef(model1)[2]   # extract the slope

Y.predict.1 <- 1 / ( 1 + exp( -( b0 + b1*X.predict ) ) )

plot(Y ~ X1,type="p",main="prediction method 1",bty="L",las=1,
     data = mydata) # now plot the raw data
lines(X.predict,Y.predict.1,col="red",lwd=2)

# *****************************************************
# PREDICTION METHOD 2 - use the predict function

# this time we need to create a new dataset which we will ask the model
# to predict values of Y from using its internal information.
data.predict <- data.frame(X1=X.predict)

# use the predict function
Y.predict.2 <- predict(model1, newdata=data.predict, type="response")

plot(Y ~ X1, type="p",main="prediction method 2",bty="L",las=1,
     data = mydata) # now plot the raw data

lines(X.predict,Y.predict.2,col="blue",lwd=2)

# *****************************************************
# now the only thing remaining is to make sure the residuals of this
# model are normally distributed. NB there are more than one kind of residual.
# For all GLMS, the predcition is that the "deviance residuals" are normally
# distributed.

qqnorm(resid(model1,type="deviance"))
qqline(resid(model1,type="deviance"),col="red",lwd=2)


# ------------------------------------------------------------------------------
# ** END OF THE 2X2 PANEL PLOTTING **
# ------------------------------------------------------------------------------


# A histogram of the data if you want
dev.new()
hist(resid(model1,type="deviance"),freq=F)

# you can superimpose the model over the horizontal boxplots if you 
# prefer that style.
# Note we plot X1~Y this time as the histrogram is flipped compared with 
# how we were plotting above, and how we would expect the response variable
# to be on the y axis
boxplot(X1~Y,horizontal=T,ylab="Y",xlab="X1",
          main="superimpose over boxes", frame.plot=T, las=1, 
        data = mydata)

# note the +1 to the Y variable as the model line is between 0 and 1
# whereas the categories that boxplot produces are 
# automatically assigned 1 and 2
lines(X.predict,Y.predict.2+1,col="blue",lwd=2)

```



