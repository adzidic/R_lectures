---
title: "Maximum Likelihood"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

Likelihood is a means to determine how likely a distribution is given some observed data. More generally, it allows us to ask how likely a model is (a model is ultimately but a distribution) given the data. The observed data are more likely to come from some models compared with others, and so we have a framework for comparing alternative models. The aspirational goal is to find a model that maximises the likelihood of the observed data being generated by it.

It is important to realise that the observed data are immutable, and instead we are trying alternative models fit to these data. However, the phrasing of this process means that we are always asking how likely is it that a particular model generated the data. We *are not* asking directly which model is the most likely. This is a subtle, but related difference. The latter question: **what is the probability of the mean of my distribution?** is only answerable using Bayesian Inference and that comes with additional assumptions that must be made (these assumptions are known as priors).

To explore maximum likelihood we need to have some data, so lets generate some normally distributed data, with an **a priori** known mean and standard deviation.

```{r simulate-data}

# the population (true) mean
mu.pop <- 5.5

# the population (true) standard deviation
sd.pop <- 1.6

# the sample size
n <- 50

# generate the data, after setting the random seed
set.seed(1)

y <- rnorm(n, mu.pop, sd.pop)

```

Of course, we can simply calculate the mean and standard deviation of these data using functions in R which themselves use mathematical equations As it happens, these equations return the maximum likelihood solution for the mean and standard deviation; but we will satisfy ourselves that this is indeed the case by doing it manually, and using an automatic solver to find the maximum likelihood solution.

To start, we should plot our data, and add on to it the sample mean and sample standard deviation.

```{r visualise-data}

# sample mean and standard deviation
# mu.samp <- mean(y)
# sd.samp <- sd(y)

hist(y)
points(y, rep(0,n), pch = "x", col = "black")
# abline(v = mu.samp, col = "red", lty = 2)

```

## Guessing some models

We could start by taking a guess at model to describe these data. We will first assume that the data are normally distributed and so we need to propose a mean and an error term, which in R is specified as a standard deviation. We can then use the likelihood function to calculate how likely the parameters given the data.

But before doing this, it is useful to visualise what our model looks like by plotting the probability density function (which is essentially the likelihood function) over the range of values around our data.


```{r guess-a-model}

# initial proposed values
mu.1 <- 6
sd.1 <- 1

# visualise this density function on the graph. We do this by asking how likely a sequence
# from say 0 to 10 for this example, and then plot the density function on the graph.

# draw the histogram as a probabbility density histogram 
# (rather than a frequency histogram)
hist(y, probability = TRUE, ylim = c(0, 0.4))

# use the curve() function to add and scale the density curve so it matches the
# scaling of the histogram.
curve(dnorm(x, mean = mu.1, sd = sd.1), add = T, col = "red")

```

Our initial guess that a normal distribution with a mean of `r mu.1` and a standard deviation of `r sd.1` seemed pretty good by visual inspecting. Based on this probability density curve, we can calculate the log-likelihood of our model (we take the log to make the maths easier, but it also rescales the data more sensibly for us) given the data. Since the probability of two events occurring simultaneously is their product (the first probability multipled by the second) we multiply the likelihood across the data, which is the same as summing across the log-liklihoods since $\log(a * b* c) = \log(a) + \log(b) + \log(c)$:

```{r first-likelihood}

# note the log = TRUE argument in call to dnorm()
like.1 <- sum(dnorm(y, mean = mu.1, sd = sd.1, log = TRUE))

```

The log-likelihood of our data given our initial parameter guesses is `round(like.1)`.

## Make some other guesses

As an excercise, we can make some other guesses, both of which we might expect **a priori** to be less likely given the data (we can make this guess based on the graph above).

```{r other-guesses}

# second guess
mu.2 <- 4
sd.2 <- 2

# third guess
mu.3 <- 7
sd.3 <- 0.5


# draw the histogram as a probabbility density histogram 
# (rather than a frequency histogram)
# I set a different y-axis limit here to make sure all 
# three of my density curves are plotted.
hist(y, probability = TRUE, ylim = c(0, 1))

# use the curve() function to add and scale the density curve so it matches the
# scaling of the histogram.
curve(dnorm(x, mean = mu.1, sd = sd.1), add = T, col = "red")
curve(dnorm(x, mean = mu.2, sd = sd.2), add = T, col = "blue")
curve(dnorm(x, mean = mu.3, sd = sd.3), add = T, col = "magenta")

```

The corresponding likelihoods for each are:

```{r other-likelihoods}

# note the log = TRUE argument in call to dnorm()
like.2 <- sum(dnorm(y, mean = mu.2, sd = sd.2, log = TRUE))
like.3 <- sum(dnorm(y, mean = mu.3, sd = sd.3, log = TRUE))

# bundle these up for pretty printing to screen
three_guesses <- data.frame( model = 1:3, 
                             mu = c(mu.1, mu.2, mu.3),
                             sd = c(sd.1, sd.2, sd.3),
                             LogLik = round(c(like.1, like.2, like.3),2))


# print out data.frame to screen nicely
knitr::kable(three_guesses)

```
The log-likelihood for our three guesses shows us clearly that model 1, has a higher (closer to positive infinity) than the others and so is the most likely of the models among the three we compared. 

The task now is to try more alternative models and find the one that maximises the likelihood.

To do this, we will make our lives easier to start by assuming that we know the standard deviation of our data (we will take it to be the standard deviation of the sample of our data `y` which is $\sigma_y = \text{sd}(y) = $`r sd.samp`) but our mean is unknown.

## Loop over alternative guesses for the mean

We can use a loop to try lots of different guesses for the mean, calculate the corresponding log-likelihood, store all the results and show us which one is the highest.

```{r loop-over-means}

# our set of means to try
mu.alternative <- seq(0, 10, 0.1)

# set up an empty vector in which to store our results
loglik_results <- numeric(length(mu.alternative))

# loop and evaluate the log-likelihood function

for (i in 1:length(mu.alternative)) {
  
  loglik_results[i] <- sum(dnorm(y, mean = mu.alternative[i], sd = sd.samp, log = TRUE))
    
}

# plot the log-likelihood of our guesses for each value of mu we tried
plot(loglik_results ~ mu.alternative, bty = "L", las = 1, type = "l")

# there is a clear maximum to this function we graphed so we can 
# find the index of the corresponding value, return the mean that generated it, 
# and add a line to the plot to illustrate it.
max_like_idx <- which.max(loglik_results)

# and the corresponding mean that is associated with this value
mu.max_like <- mu.alternative[max_like_idx]

# add the line
abline(v = mu.max_like, col = "red", lty = 2)

```

Now we obviously havent tried every possible value for the mean. This would be impossible computationally, since there are an infinite number of numbers between 5 and 6 where we expect our true mean to lie. We got pretty close though:

```{r how-close}

# The sample mean is calculated as
print(paste("Sample mean:", round(mu.samp,2)))


# our guess using our loop and maximum likelihood approach is
print(paste("Our maximum likelihood mean:", round(mu.max_like,2)))


# the difference between them is
print(paste("Difference between the esimates:", round(mu.samp - mu.max_like,2)))
```

## Use an optimiser to find the maximum

There are lots of optimiser algorithms available in R. These algorithms typically minimise a function, whereas what we want to do is maximise it. We can easily fix this by putting a negative sign in front of our log-likelihood calculation which flips it upsidedown.

Again, for simplicity we will assume that we know the standard deviation and want to estimate the mean. We will use the function `nlminb` to do our optimising. To use this function, we need to specify the "objective function", which is the function we want to optimise over. We can't just give it the `dnorm()` function as we need to flip it using the negative, and we also want to sum over the log-likelihood values. The examples section of the help section `?nlminb` provides us with a template for this excercise.

```{r optimise-mean}

# Define our own objective function based on the
# negative log likelihood as above. I have hard coded the 
# standard deviation to be equal to sd.samp

optfun <- function(par) {
  -sum(dnorm(y, mean = par[1], sd = sd.samp), log = TRUE)
}

# make an initial guess at where the mean might be
initial.mu <- 5

# optimise 
mu_optimised <- nlminb(c(initial.mu), optfun)

print(mu_optimised)

```

The optimiser has estimated the value of the mean that maximises





