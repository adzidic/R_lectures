---
title: "Maximum Likelihood"
date: "`r format(Sys.time(), '%d %B %Y')`"
author: 
 - name: Andrew L Jackson
   email: jacksoan@tcd.ie
   affiliation: School of Natural Sciences, Trinity College Dublin, Ireland
output: html_notebook
---

Likelihood is a means to determine how likely a distribution is given some observed data. More generally, it allows us to ask how likely a model is (a model is ultimately but a distribution) given the data. The observed data are more likely to come from some models compared with others, and so we have a framework for comparing alternative models. The aspirational goal is to find a model that maximises the likelihood of the observed data being generated by it.

It is important to realise that the observed data are immutable, and instead we are trying alternative models fit to these data. However, the phrasing of this process means that we are always asking how likely is it that a particular model generated the data. We *are not* asking directly which model is the most likely. This is a subtle, but related difference. The latter question: **what is the probability of the mean of my distribution?** is only answerable using Bayesian Inference and that comes with additional assumptions that must be made (these assumptions are known as priors).

To explore maximum likelihood we need to have some data, so lets generate some normally distributed data, with an **a priori** known mean and standard deviation.

```{r simulate-data}

# the population (true) mean
mu_pop <- 5.5

# the population (true) standard deviation
sd_pop <- 1.6

# the sample size
n <- 50

# generate the data, after setting the random seed
set.seed(1)

# sample n times from a gaussian distribution with mean mu.pop and sd_pop
y <- rnorm(n, mu_pop, sd_pop)

# the sample mean and standard deviation
sd_samp <- sd(y)
mu_samp <- mean(y)

```

Of course, we can simply calculate the mean and standard deviation of these data using functions in R which themselves use mathematical equations As it happens, these equations return the maximum likelihood solution for the mean and standard deviation; but we will satisfy ourselves that this is indeed the case by doing it manually, and using an automatic solver to find the maximum likelihood solution.

To start, we should plot our data, and add on to it the sample mean and sample standard deviation.

```{r visualise-data}
# histrogram of y with the points represented along the x-axis
hist(y)
points(y, rep(0,n), pch = "x", col = "black")
# abline(v = mu_samp, col = "red", lty = 2) # could add the sample mean

```

## Guessing some models

We could start by taking a guess at model to describe these data. We will first assume that the data are normally distributed and so we need to propose a mean and an error term, which in R is specified as a standard deviation. We can then use the likelihood function to calculate how likely the parameters given the data.

But before doing this, it is useful to visualise what our model looks like by plotting the probability density function (which is essentially the likelihood function) over the range of values around our data.


```{r guess-a-model}

# initial proposed values
mu_1 <- 6
sd_1 <- 1

# visualise this density function on the graph. We do this by asking how likely a sequence
# from say 0 to 10 for this example, and then plot the density function on the graph.

# draw the histogram as a probabbility density histogram 
# (rather than a frequency histogram)
hist(y, probability = TRUE, ylim = c(0, 0.4))

# use the curve() function to add and scale the density curve so it matches the
# scaling of the histogram.
curve(dnorm(x, mean = mu_1, sd = sd_1), add = T, col = "red")

```

Our initial guess that a normal distribution with a mean of `r mu_1` and a standard deviation of `r sd_1` seemed pretty good by visual inspecting. Based on this probability density curve, we can calculate the log-likelihood of our model (we take the log to make the maths easier, but it also rescales the data more sensibly for us) given the data. Since the probability of two events occurring simultaneously is their product (the first probability multipled by the second) we multiply the likelihood across the data, which is the same as summing across the log-liklihoods since $\log(a * b* c) = \log(a) + \log(b) + \log(c)$:

```{r first-likelihood}

# note the log = TRUE argument in call to dnorm()
like_1 <- sum(dnorm(y, mean = mu_1, sd = sd_1, log = TRUE))

```

The log-likelihood of our data given our initial parameter guesses is `round(like.1)`.

## Make some other guesses

As an excercise, we can make some other guesses, both of which we might expect **a priori** to be less likely given the data (we can make this guess based on the graph above).

```{r other-guesses}

# second guess
mu_2 <- 4
sd_2 <- 2

# third guess
mu_3 <- 7
sd_3 <- 0.5


# draw the histogram as a probabbility density histogram 
# (rather than a frequency histogram)
# I set a different y-axis limit here to make sure all 
# three of my density curves are plotted.
hist(y, probability = TRUE, ylim = c(0, 1))

# use the curve() function to add and scale the density curve so it matches the
# scaling of the histogram.
curve(dnorm(x, mean = mu_1, sd = sd_1), add = T, col = "red")
curve(dnorm(x, mean = mu_2, sd = sd_2), add = T, col = "blue")
curve(dnorm(x, mean = mu_3, sd = sd_3), add = T, col = "magenta")

```

The corresponding likelihoods for each are:

```{r other-likelihoods}

# note the log = TRUE argument in call to dnorm()
like_2 <- sum(dnorm(y, mean = mu_2, sd = sd_2, log = TRUE))
like_3 <- sum(dnorm(y, mean = mu_3, sd = sd_3, log = TRUE))

# bundle these up for pretty printing to screen
three_guesses <- data.frame( model = 1:3, 
                             mu = c(mu_1, mu_2, mu_3),
                             sd = c(sd_1, sd_2, sd_3),
                             LogLik = round(c(like_1, like_2, like_3),2))


# print out data.frame to screen nicely
knitr::kable(three_guesses)

```
The log-likelihood for our three guesses shows us clearly that model 1, has a higher (closer to positive infinity) than the others and so is the most likely of the models among the three we compared. 



## Loop over alternative guesses for the mean

The task now is to try more alternative models and find the one that maximises the likelihood.

To do this, we will make our lives easier to start by assuming that we know the standard deviation of our data (we will take it to be the standard deviation of the sample of our data `y` which is $\sigma_{y} = \text{sd}(y) = \\$ `r round(sd_samp,2)`) but our mean is unknown. 

We can use a loop to try lots of different guesses for the mean, calculate the corresponding log-likelihood, store all the results and show us which one is the highest.

```{r loop-over-means}

# our set of means to try
mu_alternative <- seq(0, 10, 0.1)

# set up an empty vector in which to store our results
loglik_results <- numeric(length(mu_alternative))

# loop and evaluate the log-likelihood function

for (i in 1:length(mu_alternative)) {
  
  loglik_results[i] <- sum(dnorm(y, mean = mu_alternative[i], sd = sd_samp, log = TRUE))
    
}

# plot the log-likelihood of our guesses for each value of mu we tried
plot(loglik_results ~ mu_alternative, bty = "L", las = 1, type = "l")

# there is a clear maximum to this function we graphed so we can 
# find the index of the corresponding value, return the mean that generated it, 
# and add a line to the plot to illustrate it.
max_like_idx <- which.max(loglik_results)

# and the corresponding mean that is associated with this value
mu_max_like <- mu_alternative[max_like_idx]

# add the line
abline(v = mu_max_like, col = "red", lty = 2)

```

Now we obviously havent tried every possible value for the mean. This would be impossible computationally, since there are an infinite number of numbers between 5 and 6 where we expect our true mean to lie. By moving along in steps of 0.1 (defined where we created `mu_alternative <- seq(0, 10, 0.1)`), we got pretty close though:

+ the sample mean is `r round(mu_samp,2)`
+ Our guess using our looped maximum likelihood approach is `r round(mu_max_like,2)`
+ and the difference between them is `r round(mu_samp - mu_max_like,2)`

We could go back and make `mu_alternative` as a new sequence with much smaller steps and focus our effort around `r round(mu_max_like,2)`, but algorithms capable of searching parameter space are a core component of statistical modelling and R has several we can employ.

## Use an optimiser to find the maximum

There are lots of optimiser algorithms available in R. These algorithms typically minimise a function, whereas what we want to do is maximise it. We can easily fix this by putting a negative sign in front of our log-likelihood calculation which flips it upsidedown.

Again, for simplicity we will assume that we know the standard deviation and want to estimate the mean. We will use the function `nlminb` to do our optimising. To use this function, we need to specify the "objective function", which is the function we want to optimise over. We can't just give it the `dnorm()` function as we need to flip it using the negative, and we also want to sum over the log-likelihood values. The examples section of the help section `?nlminb` provides us with a template for this excercise.

```{r optimise-mean}

# Define our own objective function based on the
# negative log likelihood as above. I have hard coded the 
# standard deviation to be equal to sd_samp

optfun <- function(par) {
  -sum(dnorm(y, mean = par[1], sd = sd_samp), log = TRUE)
}

# make an initial guess at where the mean might be
initial_mu <- 5

# optimise 
mu_optimised <- nlminb(c(initial_mu), optfun)

# print(mu_optimised)

```

The optimiser has estimated the value of the mean that maximises the likelihood and esimated a value of $\mu = \\$ `r round(mu_optimised$par,2 )`. The contents of the object `mu_optimised` returned by `nlminb()` also includes the value of the objective function (i.e. the target we are trying to hit) which is the value optfun() which is the *__lowest__* value of the *negative of the log-likelihood of the Gaussian distribution*, and hence represents the maximum of the likelihood, along with an indication how many iterations (guesses) it took to find this solution.


## Takeaway concepts

One of the key concepts to take away from this is that we now have a method to compare among multiple models for our data using Maximum Likelihood. Our model in this example has two components: 

+ its structure which in this context refers to the distribution we assumed the data come from, which here we assumed to be Gaussian $y \sim Norm(\mu, \sigma)$.
+ thereafter, our models differ by the values assigned to $\mu$ and $\sigma$

Since all the models we considered above had the same structure and number of parameters ($k=2$) we were free to compare across them all and find the values for $\mu$ and $\sigma$ that maximise the likelihood of the data. 
We proceed by calculating the log(likelihood) of the data given a particular combination of $\mu$ and $\sigma$ and try to maximimse this value. In practice, we use an optimiser which typically tries to minimise a function and so by minimising the negative of the log(likelihood) we effectively maximise both the log(likelihood) and hence the likelihood itself.

This process is very similar to the concepts applied in model selection where is an additional step required to consider comparisons among models that differ in the number of parameters. Specifically, a statistic called [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) is calculated and used as a correction to likelihood 

$$\text{AIC} = -2\log(\text{Likelihood}) + 2k$$
We explore AIC and its application to model selection in other tutorials.



